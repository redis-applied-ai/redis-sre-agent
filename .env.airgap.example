# .env.airgap.example - Configuration for air-gapped deployment
#
# Copy this file to .env and configure for your environment.
# All URLs should point to internal infrastructure (no internet access).

# =============================================================================
# REQUIRED: Redis Connection
# =============================================================================
# Your internal Redis instance with RediSearch module enabled
# This is used for agent state, task queue, and knowledge base vector storage
REDIS_URL=redis://your-internal-redis:6379/0
REDIS_PASSWORD=

# =============================================================================
# REQUIRED: LLM Configuration
# =============================================================================
# Internal LLM proxy endpoint (must be OpenAI-compatible API)
# Examples: Azure OpenAI, vLLM, Ollama, LiteLLM proxy, etc.
OPENAI_BASE_URL=https://your-internal-llm-proxy.company.com/v1
OPENAI_API_KEY=your-internal-api-key

# Model names (must match what your internal LLM proxy provides)
OPENAI_MODEL=gpt-4
OPENAI_MODEL_MINI=gpt-4
OPENAI_MODEL_NANO=gpt-4

# =============================================================================
# EMBEDDINGS: Local (Air-Gap Compatible)
# =============================================================================
# Use local HuggingFace models (pre-bundled in image, no network needed)
EMBEDDING_PROVIDER=local

# Available pre-bundled models:
#   - sentence-transformers/all-MiniLM-L6-v2 (384 dims, faster, smaller)
#   - sentence-transformers/all-mpnet-base-v2 (768 dims, better quality)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
VECTOR_DIM=384

# Alternative: Use your internal embedding API (if available)
# EMBEDDING_PROVIDER=openai
# EMBEDDING_MODEL=text-embedding-3-small
# VECTOR_DIM=1536

# =============================================================================
# OPTIONAL: Monitoring Integration
# =============================================================================
# Internal Prometheus instance for metrics queries
TOOLS_PROMETHEUS_URL=http://your-internal-prometheus:9090

# Internal Loki instance for log queries
TOOLS_LOKI_URL=http://your-internal-loki:3100

# =============================================================================
# OPTIONAL: API Security
# =============================================================================
# Master key for API authentication (leave empty to disable)
REDIS_SRE_MASTER_KEY=

# =============================================================================
# OPTIONAL: GitHub MCP Server
# =============================================================================
# Personal access token for GitHub API (required for GitHub MCP server)
# Create at: https://github.com/settings/tokens
# Required scopes: repo (for private repos) or public_repo (for public only)
GITHUB_PERSONAL_ACCESS_TOKEN=

# =============================================================================
# OPTIONAL: Image Configuration
# =============================================================================
# Override if using custom registry
SRE_AGENT_IMAGE=redis-sre-agent:airgap
# SRE_UI_IMAGE=redis-sre-agent-ui:airgap

# Port mappings
API_PORT=8080
UI_PORT=3002

# =============================================================================
# OPTIONAL: Volume Paths
# =============================================================================
# Path to config.yaml (for MCP server configuration, etc.)
CONFIG_PATH=./config.yaml

# Path to pre-built knowledge base artifacts
ARTIFACTS_PATH=./artifacts

# Skip automatic knowledge base initialization (use pre-built artifacts)
SKIP_KNOWLEDGE_INIT=false
