services:
  # Redis - Agent operational database and vector store
  redis:
    image: redis:8.2.1
    ports:
      - "7843:6379"  # Agent operational Redis
    volumes:
      - redis_data:/data
      - ./monitoring/redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    healthcheck:
      # Wait for Redis to finish loading before marking healthy
      test: ["CMD-SHELL", "redis-cli ping | grep -q PONG && redis-cli INFO persistence | grep -q 'loading:0'"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 10s
    networks:
      - sre-network

  # Redis Demo - Target instance for testing scenarios
  redis-demo:
    image: redis:8.2.1
    ports:
      - "7844:6379"  # Demo/testing Redis
    volumes:
      - redis_demo_data:/data
    labels:
      - "logging=true"  # Collected by promtail for Loki
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    networks:
      - sre-network

  # Redis Demo Replica - Follows redis-demo for replication tests
  redis-demo-replica:
    image: redis:8.2.1
    ports:
      - "7845:6379"  # Demo replica Redis
    volumes:
      - redis_demo_replica_data:/data
    command: ["redis-server", "--replicaof", "redis-demo", "6379"]
    labels:
      - "logging=true"  # Collected by promtail for Loki
    depends_on:
      redis-demo:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sre-network

  # Redis Exporter for Redis demo metrics
  redis-exporter:
    image: oliver006/redis_exporter:v1.80.2
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://redis-demo:6379
      - REDIS_EXPORTER_LOG_FORMAT=txt
    depends_on:
      redis-demo:
        condition: service_healthy
    networks:
      - sre-network


  # Redis Exporter for Agent operational Redis metrics
  redis-exporter-agent:
    image: oliver006/redis_exporter:v1.80.2
    ports:
      - "9122:9121"
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_EXPORTER_LOG_FORMAT=txt
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - sre-network

  # Pushgateway for demo metrics (Prometheus pulls from here; demos can push)
  pushgateway:
    image: prom/pushgateway:v1.11.2
    ports:
      - "9091:9091"
    networks:
      - sre-network

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:v3.5.1
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - sre-network

  # Grafana - Metrics visualization
  grafana:
    image: grafana/grafana:12.3.2
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    depends_on:
      - prometheus
      - loki
      - tempo
    networks:
      - sre-network

  # Node Exporter - System metrics
  node-exporter:
    image: prom/node-exporter:v1.10.2
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - sre-network

  # SRE Agent API
  # NOTE: Uses LiteLLM proxy for LLM calls. The OPENAI_API_KEY here is the
  # LiteLLM master key (for auth to proxy), NOT the real OpenAI key.
  # The real OpenAI key is configured only in the litellm service below.
  sre-agent:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8000"
    environment:
      - REDIS_URL=redis://redis:6379/0  # Internal container port stays 6379
      - TOOLS_PROMETHEUS_URL=http://prometheus:9090
      - TOOLS_LOKI_URL=http://loki:3100
      - GRAFANA_URL=http://grafana:3000
      - REDIS_SRE_MASTER_KEY=${REDIS_SRE_MASTER_KEY}
      # LiteLLM proxy config: point to proxy, use master key for auth
      - OPENAI_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      # OTEL: export spans to Tempo via OTLP HTTP
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4318/v1/traces
      - OTEL_RESOURCE_ATTRIBUTES=service.name=redis-sre-agent,service.version=0.1.0,deployment.environment=dev
    depends_on:
      redis:
        condition: service_healthy
      prometheus:
        condition: service_started
      litellm:
        condition: service_healthy
      tempo:
        condition: service_started  # no health check configured yet

    volumes:
      - .env:/app/.env
      - ./config.yaml:/app/config.yaml  # Mount config for hot-reload on restart
      - ./artifacts:/app/artifacts  # For data pipeline
      - ./redis_sre_agent:/app/redis_sre_agent  # Mount source code for development
      - ./tests:/app/tests  # Mount tests for development
    command: uvicorn redis_sre_agent.api.app:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - sre-network

  # SRE Agent Background Worker
  # NOTE: Uses LiteLLM proxy for LLM calls. The OPENAI_API_KEY here is the
  # LiteLLM master key (for auth to proxy), NOT the real OpenAI key.
  # The real OpenAI key is configured only in the litellm service below.
  sre-worker:
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - REDIS_URL=redis://redis:6379/0  # Internal container port stays 6379
      - REDIS_SRE_MASTER_KEY=${REDIS_SRE_MASTER_KEY}
      - TOOLS_PROMETHEUS_URL=http://prometheus:9090
      - TOOLS_LOKI_URL=http://loki:3100
      # LiteLLM proxy config: point to proxy, use master key for auth
      - OPENAI_BASE_URL=http://litellm:4000/v1
      - OPENAI_API_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      # OpenTelemetry export to Tempo
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://tempo:4318/v1/traces
      - OTEL_RESOURCE_ATTRIBUTES=service.name=redis-sre-worker,service.version=0.1.0,deployment.environment=dev
    depends_on:
      redis:
        condition: service_healthy
      litellm:
        condition: service_healthy
    volumes:
      - .env:/app/.env
      - ./config.yaml:/app/config.yaml  # Mount config for hot-reload on restart
      - ./artifacts:/app/artifacts
      - /var/run/docker.sock:/var/run/docker.sock  # Mount Docker socket for log access
      - ./redis_sre_agent:/app/redis_sre_agent  # Mount source code for development
      - ./tests:/app/tests  # Mount tests for development
      - uv_cache:/root/.cache/uv  # Persist UV cache for MCP server deps
    # Run the worker directly from the project virtualenv instead of via
    # `uv run` to avoid querying /app/.venv/bin/python3 at runtime.
    command: redis-sre-agent worker start
    # Disable healthcheck for worker (it doesn't run a web server)
    healthcheck:
      disable: true
    networks:
      - sre-network

  # GitHub MCP Server - Exposes GitHub tools via MCP
  # This runs the GitHub MCP server behind an SSE/HTTP proxy so the sre-worker
  # can connect to it without needing Docker-in-Docker permissions.
  github-mcp:
    image: ghcr.io/sparfenyuk/mcp-proxy:v0.11.0
    ports:
      - "8082:8082"
    environment:
      - GITHUB_PERSONAL_ACCESS_TOKEN=${GITHUB_PERSONAL_ACCESS_TOKEN}
    command: >
      --pass-environment
      --port=8082
      --host=0.0.0.0
      docker run -i --rm -e GITHUB_PERSONAL_ACCESS_TOKEN ghcr.io/github/github-mcp-server
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - sre-network
    profiles:
      - mcp  # Start with: docker compose --profile mcp up

  # SRE Agent MCP Server - Exposes agent capabilities via Model Context Protocol
  # Connect Claude to this via: Settings > Connectors > Add Custom Connector
  # HTTP: http://localhost:8081/mcp
  # HTTPS: https://localhost:8450/mcp (requires running scripts/generate-mcp-certs.sh first)
  sre-mcp:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
    environment:
      - REDIS_URL=redis://redis:6379/0
      - REDIS_SRE_MASTER_KEY=${REDIS_SRE_MASTER_KEY}
      - TOOLS_PROMETHEUS_URL=http://prometheus:9090
      - TOOLS_LOKI_URL=http://loki:3100
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - .env:/app/.env
      - ./redis_sre_agent:/app/redis_sre_agent
    command: uv run redis-sre-agent mcp serve --transport http --host 0.0.0.0 --port 8081
    networks:
      - sre-network
    profiles:
      - mcp  # Start with: docker compose --profile mcp up
      - ssl  # Or with SSL: docker compose --profile ssl up

  # MCP SSL Proxy - HTTPS termination for MCP server
  # Run scripts/generate-mcp-certs.sh first to generate self-signed certs
  sre-mcp-ssl:
    image: nginx:alpine
    ports:
      - "8450:443"
    volumes:
      - ./monitoring/nginx/mcp-ssl.conf:/etc/nginx/conf.d/default.conf:ro
      - ./monitoring/nginx/certs:/etc/nginx/certs:ro
    depends_on:
      - sre-mcp
    networks:
      - sre-network
    profiles:
      - ssl  # Only start with: docker compose --profile ssl up

  # SRE Agent UI
  sre-ui:
    build:
      context: ./ui
      dockerfile: Dockerfile
      target: development
    ports:
      - "3002:3000"
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://sre-agent:8080/api/v1
    depends_on:
      - sre-agent
    volumes:
      - ./ui:/app
      - /app/node_modules  # Anonymous volume for node_modules
    networks:
      - sre-network


  # LiteLLM Database - PostgreSQL for UI and usage tracking
  litellm-db:
    image: postgres:16-alpine
    environment:
      - POSTGRES_USER=litellm
      - POSTGRES_PASSWORD=litellm
      - POSTGRES_DB=litellm
    volumes:
      - litellm_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U litellm"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - sre-network

  # LiteLLM Proxy - LLM gateway for OpenAI, Anthropic, etc.
  #
  # TOKEN FLOW:
  # 1. LiteLLM receives the REAL provider API keys (OPENAI_API_KEY, ANTHROPIC_API_KEY)
  # 2. Clients (sre-agent, sre-worker) authenticate to LiteLLM using LITELLM_MASTER_KEY
  # 3. Clients set OPENAI_BASE_URL=http://litellm:4000/v1 and OPENAI_API_KEY=$LITELLM_MASTER_KEY
  #
  # In .env you need:
  #   OPENAI_API_KEY=sk-your-real-openai-key    # Real key, used ONLY by LiteLLM
  #   LITELLM_MASTER_KEY=sk-1234                # Proxy auth key, used by clients
  #
  litellm:
    image: ghcr.io/berriai/litellm:v1.81.0-stable
    ports:
      - "4000:4000"
    environment:
      # Real provider API keys - LiteLLM uses these to call OpenAI/Anthropic
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      # Master key for client authentication (sre-agent, sre-worker use this)
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      # UI credentials
      - UI_USERNAME=${LITELLM_UI_USERNAME:-admin}
      - UI_PASSWORD=${LITELLM_UI_PASSWORD:-admin}
      - DATABASE_URL=postgresql://litellm:litellm@litellm-db:5432/litellm
    volumes:
      - ./monitoring/litellm/config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    depends_on:
      litellm-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4000/health/liveliness')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - sre-network

  # Tempo - Traces backend (OTLP receiver + query)
  tempo:
    image: grafana/tempo:v2.10.0
    user: "0:0"  # run as root to create/write under mounted volume
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    ports:
      - "3200:3200"   # Tempo HTTP API (querier)
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
    volumes:
      - tempo_data:/tmp/tempo
      - ./monitoring/tempo/tempo-config.yaml:/etc/tempo/tempo.yaml:ro
    networks:
      - sre-network



  # Loki - Log aggregation
  loki:
    image: grafana/loki:3.5.0
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - loki_data:/loki
      - ./monitoring/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
    networks:
      - sre-network

  # Promtail - Log shipping to Loki
  promtail:
    image: grafana/promtail:3.5.0
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./monitoring/loki/promtail-config.yaml:/etc/promtail/config.yml:ro
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - loki
    networks:
      - sre-network

networks:
  sre-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16
          ip_range: 172.28.1.0/24


volumes:
  redis_data:
  redis_demo_data:
  redis_demo_replica_data:
  prometheus_data:
  grafana_data:
  loki_data:
  tempo_data:
  litellm_db_data:
  uv_cache:  # Persist UV cache for MCP server dependencies
